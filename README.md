# Hippo
Mesos framework for eating tasks off queues and running them as docker containers.  One-off tasks are also supported.

The two abstractions used are "Queues" and "Tasks".  Queues are definitions for things that generate tasks, and tasks are
the records for the actual mesos tasks that were triggered.  One-off tasks can also be generated outside of a queue.

For tasks created from queues, the queue data is made available to the task as both replacement and ENV variables.  Two
ENV vars are set:
1.  HIPPO_DATA - raw queue data
2.  HIPPO_DATA_BASE64 - queue data base64 encoded

Additionally, you can use $HIPPO_DATA and $HIPPO_DATA_BASE64 in your other env var values and cmd string and they will be
replaced with the actual data at run time. See the queue types below for what types of data will be present, as this varies
by queue type.  Note that for batched runs, the data in these variables will be separated by the batch separator.


## API

**Queues**

```javascript
{
    // Unique ID for the queue, generated by Hippo during creation
    "id": "1111-11111-1111-1111",
    // Container definition follows normal marathon formatting and is required.
    // Each task will create a container with these settings.
    "container": {
        "docker": {
            "image" : "myrepo/myimage:mytag",
            // network is optional, defaults to bridge
            "network": "BRIDGE"
        },
        // Volumes work just like marathon volume definitions, optional
        "volumes": [
            {
                "hostPath":"/some/host/path",
                "containerPath":"/some/container/path",
                "mode":"RW"
            }
        ]
    },
    // Mesos memory setting, required
    "mem": 32,
    // Mesos cpu setting, required
    "cpus": 0.1,
    // Command to run, required
    "cmd": "mycommand --with --my --args"
    // Marathon formatted ENV vars to pass to container, optional
    "env": {
        "MYVARNAME":"MYVARVALUE"
    },
    // Marathon formatted contraints, optional
    "constraints": [
        ["rack","EQUAL","rack1"]
    ],
    // Number of retries if mesos result is TASK_FAILED, defaults to 0, optional
    "task_retries": 0,
    // Number of retries if task fails to run but it's the systems fault, defaults to 2, optional
    "system_retries": 2,
    // Definition for the queue data source
    "queue": {
        // The type of the queue, see Queue Types documentation for possibilites
        "type": "sqs",
        // Label for this queue definition, info only
        "name": "myQueueName",
        // Number of tasks that can be processed concurrently for this queue, defaults to 10000, optional
        "max_concurrent": 10,
        // Number of tasks to group if more than one task is created from the data source at once, defaults to 1
        "batch_size": 1,
        // Character used to join multiple tasks strings into one batch string
        "batch_separator": "|",
        // Number of seconds between polls of the data source, defaults to 60
        "frequency_seconds": 60,
        // Unix timestamp for last time the data source was polled, not needed on creation
        "last_run_tstamp": 0,
        // Status of the queue, can be "ENABLED" or "DISABLED
        "status": "ENABLED",
        // Settings for the specific queue type, will vary
        "sqs": {
            "awskey":"someKey",
            "awssecret":"someSecret",
            "awsregion":"someRegion",
            "queuename":"someSqsQueueName"
        }
    }
}
```

`/queues/`

- GET, POST, PUT
- List existing queues, or create a new queue

`/queues/:queueId/`

- GET, POST, PUT, DELETE
- CRUD for a single queue

`/queues/:queueId/toggle/`

- GET
- Enables/Disables a queue


`/queuetypes/`

- GET
- Returns list of queue types enabled on this hippo server

**Tasks**

```javascript
{
    // Unique ID for the task family, generated by Hippo during creation if not provided
    "id": "myId",
    // Id created for the mesos task, generated by the system
    "mesos_id": "myId.11111-11111-11111-11111",
    // Container definition follows normal marathon formatting and is required.
    // Each task will create a container with these settings.
    "container": {
        "docker": {
            "image" : "myrepo/myimage:mytag",
            // network is optional, defaults to bridge
            "network": "BRIDGE"
        },
        // Volumes work just like marathon volume definitions, optional
        "volumes": [
            {
                "hostPath":"/some/host/path",
                "containerPath":"/some/container/path",
                "mode":"RW"
            }
        ]
    },
    // Mesos memory setting, required
    "mem": 32,
    // Mesos cpu setting, required
    "cpus": 0.1,
    // Command to run, required
    "cmd": "mycommand --with --my --args"
    // Marathon formatted ENV vars to pass to container, optional
    "env": {
        "MYVARNAME":"MYVARVALUE"
    },
    // Marathon formatted contraints, optional
    "constraints": [
        ["rack","EQUAL","rack1"]
    ],
    // Number of retries if mesos result is TASK_FAILED, defaults to 0, optional
    "task_retries": 0,
    // Number of retries if task fails to run but it's the systems fault, defaults to 2, optional
    "system_retries": 2,
    // Number of tasks that can be processed concurrently for tasks with the same ID, defaults to 10, optional
    "max_concurrent": 10
}
```

`/tasks/`

- GET, POST
- List recent tasks, or create a new one-off task

`/tasks/:taskId/`

- GET, DELETE
- Get info on a task, or delete that task record

`/tasks/:taskId/kill/`

- GET, POST
- Kill a running task


## Queue Types

- AWS SQS
  - namespace: sqs
  - hippo_data type: body of sqs message
  - parameters:
    - awskey
    - awssecret
    - awsregion
    - queuename
- AWS S3
  - namespace: s3bucket
  - hippo_data type: s3 keyname
  - parameters:
    - awskey
    - awssecret
    - awsregion
    - bucket_name
    - earliest_unix_tstamp
- Redis Queue
  - namespace: redis
  - hippo_data type: body of redis queue message
  - parameters:
    - host
    - port
    - db
    - name
- SQL Query
  - namespace: sqlquery
  - hippo_data type: json encoded list of columns for each row
  - parameters:
    - conn_string
    - query
    - filter_field
    - filter_val
- Cron String
  - namespace: cron
  - hippo_data type: unix timestamp
  - parameters:
    - cronstring
    - maxbacklog

## Architecture

Hippo UI/API -> redis <- Hippo Workers <--> Queue Data Sources and Mesos
