# Hippo
Mesos framework for eating tasks off queues and running them as docker containers.  One-off tasks are also supported.

The two abstractions used are "Queues" and "Tasks".  Queues are definitions for things that generate tasks, and tasks are
the records for the actual mesos tasks that were triggered.  One-off tasks can also be generated outside of a queue.


## API

**Queues**

```json
{
    // Unique ID for the queue, generated by Hippo during creation
    "id": "1111-11111-1111-1111",
    // Container definition follows normal marathon formatting and is required.
    // Each task will create a container with these settings.
    "container": {
        "docker": {
            "image" : "myrepo/myimage:mytag",
            // network is optional, defaults to bridge
            "network": "BRIDGE"
        },
        // Volumes work just like marathon volume definitions, optional
        "volumes": [
            {
                "hostPath":"/some/host/path",
                "containerPath":"/some/container/path",
                "mode":"RW"
            }
        ]
    },
    // Mesos memory setting, required
    "mem": 32,
    // Mesos cpu setting, required
    "cpus": 0.1,
    // Command to run, required
    "cmd": "mycommand --with --my --args"
    // Marathon formatted ENV vars to pass to container, optional
    "env": {
        "MYVARNAME":"MYVARVALUE"
    },
    "constraints": {
        "type":"list",
        "schema": {
            "type":"list"
        }
    },
    "max_concurrent": {
        "type":"integer",
        "min":0,
        "max":64000,
    },
    "task_retries": {
        "type":"integer",
        "min":0,
        "max":100
    },
    "system_retries": {
        "type":"integer",
        "min":0,
        "max":100
    },
    "queue": {
        "type": "dict",
        "required":True,
        "schema": {
            "type": {
                "type":"string",
                "required":True,
                "empty": False
            },
            "name": {
                "type":"string",
                "required":True,
                "empty": False
            },
            "max_concurrent": {
                "type":"integer",
                "min":0,
                "max":64000,
            },
            "batch_size": {
                "type":"integer",
                "min":1,
                "max":1000,
            },
            "batch_separator": {
                "type":"string",
            },
            "frequency_seconds": {
                "type":"integer",
                "min":0,
                "max":86400,
            },
            "last_run_tstamp": {
                "type":"integer"
            },
            "status": {
                "type":"string",
                "allowed":["ENABLED","DISABLED"]
            }


```

`/queues/`

- GET, POST, PUT
- Call the function designated with :myFunctionName.  Note that :callPath is not required, but anything provided will be sent along to the function in the request object.  Any GET variables sent will be included in the request object, and headers will also be provided as a parameter to the function.
- Defaults to asynchronous, but including a variable titled "hambda_async" will cause the call to be asynchronous, e.g.
`/:myFunctionName/call/?hambda_async=true`
- If called synchronously, will return whatever web payload the function returns (if any)
- If called async, will return a json object including a UUID for calling to get the results back.

`/queues/:queueId/`

- GET, POST, PUT, DELETE
- CRUD for a single queue

`/queues/:queueId/toggle/`

- GET
- Enables/Disables a queue


`/queuetypes/`

- GET
- Returns list of queue types enabled on this hippo server

**Tasks**

```json

```

`/:myFunctionName/result/:uuid`

- GET
- :uuid parameter is the same as the UUID provided in an async call
- Will return a json object with all information about the status of the call and any result payload



## Queue Types


## Architecture

Hippo UI/API -> redis <- Hippo Workers <--> Mesos
                                     \
                                      -> Queue Data Sources
